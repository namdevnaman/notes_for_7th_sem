# **MCMC Methods: Gibbs Sampling and Metropolis-Hastings**

---

## **Introduction to MCMC (Markov Chain Monte Carlo) Methods**
- MCMC methods are a class of algorithms used to approximate the probability distribution of a complex system by generating samples from it.
- **Goal**: To approximate distributions or compute expectations where direct computation is intractable due to high dimensionality or complexity.
- **Key Idea**: Use a Markov chain to generate samples that are asymptotically distributed according to the target probability distribution.

### **Applications**:
1. Bayesian inference.
2. High-dimensional integration.
3. Generating samples from complex posterior distributions in machine learning and statistical modeling.

---

## **Key Concepts in MCMC**

1. **Markov Chain**:
   - A sequence of random variables where the next state depends only on the current state (memoryless property).
   - Transition probabilities determine how the chain moves between states.

2. **Stationary Distribution**:
   - A probability distribution that remains unchanged as the Markov chain evolves over time.
   - MCMC ensures that the stationary distribution matches the target distribution.

3. **Burn-In Period**:
   - The initial iterations of the Markov chain are discarded to ensure the samples are from the stationary distribution.

4. **Convergence**:
   - The Markov chain eventually converges to the target distribution after sufficient iterations.

---

## **Gibbs Sampling**
### **Overview**:
- A specific MCMC method where samples are generated by iteratively sampling from the **conditional distributions** of each variable.
- **Goal**: Efficiently sample from multivariate distributions when the joint distribution is complex, but the conditionals are simple.

### **Algorithm**:
1. Start with an initial guess for all variables \( X = (X_1, X_2, \dots, X_n) \).
2. Update each variable \( X_i \) by sampling from its conditional distribution:
   \[
   X_i^{(t+1)} \sim P(X_i | X_1^{(t+1)}, \dots, X_{i-1}^{(t+1)}, X_{i+1}^{(t)}, \dots, X_n^{(t)})
   \]
3. Repeat this process for all variables until convergence.

### **Advantages**:
- Simple and easy to implement.
- Effective when conditional distributions are known and easy to sample from.

### **Limitations**:
- Slow convergence in high-dimensional problems or when variables are highly correlated.
- Requires knowledge of conditional distributions.

### **Example**:
- **Target Distribution**:
  \[
  P(X, Y) \propto e^{-(X^2 + Y^2 + XY)}
  \]
- **Conditional Distributions**:
  - \( P(X|Y) \sim \text{Normal}(-Y/2, 1) \)
  - \( P(Y|X) \sim \text{Normal}(-X/2, 1) \)
- Gibbs sampling alternates between these conditionals to generate samples.

---

## **Metropolis-Hastings Algorithm**
### **Overview**:
- A more general MCMC method that generates samples from a target distribution by proposing new states and accepting or rejecting them based on a criterion.
- **Key Feature**: Does not require knowing the normalizing constant of the target distribution.

### **Algorithm**:
1. **Initialize**:
   - Start with an initial state \( X_0 \).
2. **Propose a New State**:
   - Generate a candidate state \( X^* \) from a proposal distribution \( q(X^* | X_t) \).
3. **Compute the Acceptance Ratio**:
   \[
   r = \frac{P(X^*) q(X_t | X^*)}{P(X_t) q(X^* | X_t)}
   \]
   - \( P(X) \): Target distribution.
   - \( q(X^* | X_t) \): Proposal distribution.
4. **Accept or Reject**:
   - Accept the new state \( X^* \) with probability:
     \[
     \alpha = \min(1, r)
     \]
   - If accepted, set \( X_{t+1} = X^* \); otherwise, set \( X_{t+1} = X_t \).
5. Repeat until sufficient samples are generated.

### **Advantages**:
- Can be used when conditional distributions are unknown or difficult to sample from.
- Flexible: Works with various proposal distributions.

### **Limitations**:
- Efficiency depends on the choice of proposal distribution \( q(X^* | X_t) \).
- May take longer to converge if the proposal distribution is poorly chosen.

---

### **Comparison of Gibbs Sampling and Metropolis-Hastings**
| **Aspect**                | **Gibbs Sampling**                                          | **Metropolis-Hastings**                                     |
|---------------------------|------------------------------------------------------------|------------------------------------------------------------|
| **Requirement**           | Conditional distributions must be known.                  | Requires only the target distribution (up to a constant).  |
| **Efficiency**            | Faster when conditional distributions are simple.          | Slower for high-dimensional or complex distributions.      |
| **Flexibility**           | Limited to cases with known conditionals.                 | Highly flexible; can use any proposal distribution.        |

---

## **Example of Metropolis-Hastings**
### **Target Distribution**:
- \( P(X) \propto e^{-X^2} \) (Gaussian distribution).

### **Proposal Distribution**:
- \( q(X^* | X_t) = \text{Uniform}(X_t - 1, X_t + 1) \).

### **Steps**:
1. Initialize \( X_0 = 0 \).
2. Propose \( X^* = X_t + \epsilon \), where \( \epsilon \sim \text{Uniform}(-1, 1) \).
3. Compute acceptance ratio:
   \[
   r = \frac{P(X^*)}{P(X_t)}
   \]
   Since the proposal distribution is symmetric, \( q(X^* | X_t) = q(X_t | X^*) \), so it cancels out.
4. Accept \( X^* \) with probability:
   \[
   \alpha = \min(1, r)
   \]
5. Repeat to generate samples.

---

## **Applications of MCMC Methods**
1. **Bayesian Inference**:
   - Compute posterior distributions in Bayesian models.
2. **Physics and Chemistry**:
   - Simulating molecular systems and physical processes.
3. **Machine Learning**:
   - Sampling from posterior distributions in probabilistic models.
4. **Optimization**:
   - Solving non-convex optimization problems.
